import os
import numpy as np
from tensorflow import keras
from pyrsgis import raster
from pyrsgis.convert import array_to_table
from pyrsgis.convert import changeDimension
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score
from tensorflow.keras.layers import Dense, Activation,Dropout 
import matplotlib.pyplot as plt
import pandas as pd

# Change the directory to where the data is stored
os.chdir(r"path_to_data_directory")

# Assign file names
debris_image = 'image1.tif'
debris_label = 'label1.tif'
prediction = 'test.tif'

# Enter the number of images to train from 1 to n
n = 170

# Loop through all images and labels to form training data
for i in range(1,n):
    if(i==1):
        # Read the rasters as array
        ds1, featuresdebris = raster.read(debris_image, bands='all')
        ds2, debris_label = raster.read(debris_label)
        ds3, prediction = raster.read(prediction, bands='all')
        # Clean the labelled data to replace NoData values by zero
        debris_label = (debris_label == 1).astype(int)
        # Reshape the array to single-dimensional array
        featuresdebris = changeDimension(featuresdebris)
        debris_label = changeDimension(debris_label)
        prediction = changeDimension(prediction)
        nBands = featuresdebris.shape[1]
    else:
        debris_image2 = 'image'+str(i)+'.tif'
        debris_label2 = 'label'+str(i)+'.tif'
        ds12, featuresdebris2 = raster.read(debris_image2, bands='all')
        ds22, debris_label2 = raster.read(debris_label2)
        debris_label2 = (debris_label2 == 1).astype(int)
        featuresdebris2 = changeDimension(featuresdebris2)
        debris_label2 = changeDimension (debris_label2)
        featuresdebris = np.vstack((featuresdebris,featuresdebris2))
        debris_label = np.hstack((debris_label,debris_label2))

# Check the file paths and if files exist
print("Checking file paths:")
print("Debris image file path:", debris_image2)
print("Debris label file path:", debris_label2)
print("\nChecking if files exist:")
print("Debris image file exists:", os.path.exists(debris_image2))
print("Debris label file exists:", os.path.exists(debris_label2))

# Split testing and training datasets
xTrain, xTest, yTrain, yTest = train_test_split(featuresdebris, debris_label, test_size=0.3, random_state=100)

# Normalize the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(xTest)
xTrain, xTest = scaler.transform(xTrain), scaler.transform(xTest)
prediction = scaler.transform(prediction)

# Reshape the data
xTrain = xTrain.reshape((xTrain.shape[0], 1, xTrain.shape[1]))
xTest = xTest.reshape((xTest.shape[0], 1, xTest.shape[1]))
prediction = prediction.reshape((prediction.shape[0], 1, prediction.shape[1]))

# Define the model
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(1, nBands)),
    keras.layers.Dense(1024, activation='tanh'),
    keras.layers.Dense(512, activation='tanh'),
    keras.layers.Dense(256, activation='tanh'),
    keras.layers.Dense(128, activation='tanh'),
    keras.layers.Dense(64, activation='tanh'),
    keras.layers.Dense(32, activation='tanh'),
    keras.layers.Dense(2, activation='softmax')
])

model.summary()

# Compile the model
opt= keras.optimizers.Adam(learning_rate=0.0001,decay=1e-6)
model.compile(optimizer=opt, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# Train the model
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)
history = model.fit(xTrain, yTrain, epochs=1, validation_data=(xTest, yTest), verbose=1, callbacks=[early_stop], batch_size=2000)

# Plot training and validation loss/accuracy
plt.figure(figsize=(10,10), dpi=30)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.ylabel('loss (%)')
plt.xlabel('Epoch')
plt.legend(['Train loss', 'Test loss'], loc='upper right')
plt.figure(figsize=(10,10), dpi=30)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylabel('accuracy (%)')
plt.xlabel('Epoch')
plt.legend(['Train accuracy', 'Test accuracy'], loc='lower right')

# Export training history to CSV
hist_df = pd.DataFrame(history.history) 
hist_csv_file = 'history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

# Predict on test data
yTestPredicted = model.predict(xTest)
yTestPredicted = yTestPredicted[:,1]
yTestPredicted = (yTestPredicted>0.5).astype(int)

# Calculate and display error metrics
cMatrix = confusion_matrix(yTest, yTestPredicted)
pScore = precision_score(yTest, yTestPredicted)
rScore = recall_score(yTest, yTestPredicted)
print("Confusion matrix:\n", cMatrix)
print("\nP-Score: %.3f, R-Score: %.3f" % (pScore, rScore))

# Predict on new data and export the probability raster
predicted = model.predict(prediction)
predicted = predicted[:,1]
prediction = np.reshape(predicted, (ds3.RasterYSize, ds3.RasterXSize))
outFile = 'lake_estimation1.tif'
raster.export(prediction, ds3, filename=outFile, dtype='float')

# Save the model
model.save('save_model.h5')

# Apply median filter to the probability raster and save as a new raster file
with rasterio.open('lake_estimation1.tif') as src:
    raster_data = src.read(1)
    filtered_data = median_filter(raster_data, size=5)
    profile = src.profile
    profile.update(nodata=0)
    with rasterio.open('DEBRIS_MAP_post1.tif', 'w', **profile) as dst:
        dst.write(filtered_data, 1)
